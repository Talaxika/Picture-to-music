{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gEaO-WxRkrLB"
      },
      "outputs": [],
      "source": [
        "# First, upload the dataset by following these commands:\n",
        "# Go to https://drive.google.com\n",
        "# Navigate to My Drive\n",
        "# Click \"New\" â†’ \"File Upload\"\n",
        "# Upload deam_project_data.zip, which is downloaded from https://github.com/Talaxika/Picture-to-music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xevn-vm7Z4gp"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-46iJLqVe7-V"
      },
      "outputs": [],
      "source": [
        "!pip install librosa soundfile --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOcjqU99UuqF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from collections import Counter\n",
        "from random import sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TmnU6E6lJkD"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
        "    print(\"ðŸ“Ž Please upload your kaggle.json API key file\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # Upload kaggle.json\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsECoTM2lMo6"
      },
      "outputs": [],
      "source": [
        "dataset_path = kagglehub.dataset_download(\"magdawjcicka/emotic\")\n",
        "img_dir = f\"{dataset_path}/img_arrs\"\n",
        "print(\"EMOTIC image data path:\", img_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZsMOno6lPm_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHSiQXD8cq2r"
      },
      "outputs": [],
      "source": [
        "# If help needed:\n",
        "!ls \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB_-4TSsp5R4"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"/content/drive/MyDrive/project_data.zip\" -d \"/content/project_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPg96xyCiwbj"
      },
      "outputs": [],
      "source": [
        "audio_dir = \"/content/project_data/music\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPPRGPnhdP4r"
      },
      "outputs": [],
      "source": [
        "# If help needed:\n",
        "!ls /content/project_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a0A4K0bok1F"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/project_data/cleaned_train_matching.txt\", sep=' ', header=None, names=[\"audio_id\", \"image_file\", \"score\"])\n",
        "df_val = pd.read_csv(\"/content/project_data/cleaned_val_matching.txt\", sep=' ', header=None, names=[\"audio_id\", \"image_file\", \"score\"])\n",
        "df_test = pd.read_csv(\"/content/project_data/cleaned_test_matching.txt\", sep=' ', header=None, names=[\"audio_id\", \"image_file\", \"score\"])\n",
        "\n",
        "print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz6_XLJsThpr"
      },
      "outputs": [],
      "source": [
        "class ImageAudioDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, audio_dir, sr=44100, max_sec=50):\n",
        "        \"\"\"\n",
        "        sr : int, optional\n",
        "            Target sampling rate for audio files. All audio will be resampled to this rate. Default is 44100 Hz.\n",
        "        max_sec : int, optional\n",
        "            Maximum duration of audio in seconds. Audio will be truncated or zero-padded to exactly `sr * max_sec` samples. Default is 50 seconds.\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.audio_dir = audio_dir\n",
        "        self.sr = sr\n",
        "        self.max_len = sr * max_sec  # 50 seconds = 2,205,000 samples\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.img_dir, row[\"image_file\"])\n",
        "        img = np.load(img_path).astype(np.float32) / 255.0\n",
        "        img = torch.tensor(img).permute(2, 0, 1)  # [H, W, C] â†’ [C, H, W]\n",
        "\n",
        "        # Load audio\n",
        "        audio_id = row[\"audio_id\"].split(\"-\")[0]\n",
        "        audio_path = os.path.join(self.audio_dir, f\"{audio_id}.mp3\")\n",
        "        y, _ = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "        # Pad or truncate to exactly 50 seconds\n",
        "        if len(y) < self.max_len:\n",
        "            y = np.pad(y, (0, self.max_len - len(y)), mode='constant')\n",
        "        else:\n",
        "            y = y[:self.max_len]\n",
        "\n",
        "        audio_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"audio_raw\": audio_tensor,\n",
        "            \"score\": torch.tensor(row[\"score\"]).float(),\n",
        "            \"audio_id\": row[\"audio_id\"]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIe_M8McUnZz"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(ImageAudioDataset(df_train, img_dir, audio_dir), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(ImageAudioDataset(df_val, img_dir, audio_dir), batch_size=32)\n",
        "test_loader = DataLoader(ImageAudioDataset(df_test, img_dir, audio_dir), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZO10Eg_ZpSw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(df_train['score'], bins=50, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of Matching Scores (Train Set)\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Score stats:\")\n",
        "print(\"Min:\", df_train['score'].min())\n",
        "print(\"Max:\", df_train['score'].max())\n",
        "print(\"Mean:\", round(df_train['score'].mean(), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixtET0p_ZtjL"
      },
      "outputs": [],
      "source": [
        "for name, df in zip(['train', 'val', 'test'], [df_train, df_val, df_test]):\n",
        "    missing = sum(not os.path.exists(os.path.join(img_dir, fname)) for fname in df[\"image_file\"])\n",
        "    print(f\"Missing image files in {name} set: {missing} out of {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnJtWF_4fAcf"
      },
      "outputs": [],
      "source": [
        "audio_ids = set(os.path.splitext(f)[0] for f in os.listdir(audio_dir) if f.endswith(\".mp3\"))\n",
        "\n",
        "# Get numeric part of audio_id from matching files (e.g. \"2009-114\" â†’ \"2009\")\n",
        "def extract_numeric_audio_id(audio_id):\n",
        "    return audio_id.split(\"-\")[0]\n",
        "\n",
        "matching_audio_ids = set(\n",
        "    extract_numeric_audio_id(aid)\n",
        "    for aid in pd.concat([df_train, df_val, df_test])[\"audio_id\"]\n",
        ")\n",
        "\n",
        "# Compare actual vs expected\n",
        "missing_mp3s = matching_audio_ids - audio_ids\n",
        "extra_mp3s = audio_ids - matching_audio_ids\n",
        "\n",
        "print(\"Total unique numeric audio_ids in matchings:\", len(matching_audio_ids))\n",
        "print(\"Total uploaded audio files:\", len(audio_ids))\n",
        "print(\"Missing audio files:\", len(missing_mp3s))\n",
        "print(\"Extra audio files (not used):\", len(extra_mp3s))\n",
        "if missing_mp3s:\n",
        "    print(\"Example missing:\", list(missing_mp3s)[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_pf6Op5fGM8"
      },
      "outputs": [],
      "source": [
        "SAMPLE_FILES_LEN = min(150, len(audio_ids))\n",
        "sampled_ids = sample(list(audio_ids), SAMPLE_FILES_LEN)\n",
        "\n",
        "print(f\"\\nNumber of sampled audio files: {len(sampled_ids)}\")\n",
        "\n",
        "durations = []\n",
        "\n",
        "for aid in sampled_ids:\n",
        "    path = os.path.join(audio_dir, f\"{aid}.mp3\")\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=None)\n",
        "        durations.append(len(y) / sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {aid}.mp3:\", e)\n",
        "\n",
        "print(\"\\nSampled duration stats:\")\n",
        "print(\"Avg:\", round(np.mean(durations), 2), \"s\")\n",
        "print(\"Min:\", round(np.min(durations), 2), \"s\")\n",
        "print(\"Max:\", round(np.max(durations), 2), \"s\")\n",
        "print(\"Median:\", round(np.median(durations), 2), \"s\")\n",
        "\n",
        "plt.hist(durations, bins=15, color='orchid')\n",
        "plt.title(\"Sampled Audio Duration Distribution\")\n",
        "plt.xlabel(\"Duration (seconds)\")\n",
        "plt.ylabel(\"Number of Clips\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3anvdPoUgaSz"
      },
      "outputs": [],
      "source": [
        "sr_list = []\n",
        "failures = []\n",
        "\n",
        "for fname in os.listdir(audio_dir):\n",
        "    if fname.endswith(\".mp3\"):\n",
        "        path = os.path.join(audio_dir, fname)\n",
        "        try:\n",
        "            info = sf.SoundFile(path)\n",
        "            sr_list.append(info.samplerate)\n",
        "        except Exception as e:\n",
        "            failures.append(fname)\n",
        "\n",
        "# Count occurrences of each SR\n",
        "sr_counter = Counter(sr_list)\n",
        "\n",
        "# Print summary\n",
        "print(\"Sampling Rate Distribution (All Files):\")\n",
        "for rate, count in sr_counter.items():\n",
        "    print(f\"{rate} Hz: {count} files\")\n",
        "\n",
        "print(f\"\\nTotal analyzed: {len(sr_list)}\")\n",
        "print(f\"Failed to read: {len(failures)}\")\n",
        "if failures:\n",
        "    print(\"Example failed files:\", failures[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVLAk7xmlgRC"
      },
      "outputs": [],
      "source": [
        "# Use:\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(\"Image:\", batch[\"image\"].shape)        # [B, 3, 224, 224]\n",
        "print(\"Audio:\", batch[\"audio_raw\"].shape)    # [B, sr * max_sec]\n",
        "print(\"Score:\", batch[\"score\"].shape)        # [B]\n",
        "print(\"IDs:\", batch[\"audio_id\"])             # List of IDs\n",
        "\n",
        "train_loader\n",
        "test_loader\n",
        "val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5quHWdVGt4I"
      },
      "source": [
        "**MODEL** code (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel, ViTImageProcessor, ClapModel, ClapProcessor\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# TODO: Maybe compare with a resnet\n",
        "class ImageEncoder:\n",
        "    def __init__(self, vit_version):\n",
        "        # Load the model and the class that the inputs go through\n",
        "        self.model = ViTModel.from_pretrained(vit_version)\n",
        "        self.preprocessor = ViTImageProcessor.from_pretrained(vit_version)\n",
        "\n",
        "    def encode(self, data, no_grad):\n",
        "        vit_inputs = self.preprocessor(images = data, return_tensors=\"pt\", do_normalize=True, do_convert_rgb=True, do_rescale=True, do_resize=True)\n",
        "        if no_grad:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**vit_inputs)\n",
        "        else:\n",
        "            outputs = self.model(**vit_inputs)\n",
        "        return outputs.last_hidden_state\n",
        "\n",
        "class AudioEncoder:\n",
        "    def __init__(self, clap_version):\n",
        "        # Load the model and the class that the inputs go through\n",
        "        self.model = ClapModel.from_pretrained(clap_version)\n",
        "        self.preprocessor = ClapProcessor.from_pretrained(clap_version)\n",
        "\n",
        "    def encode(self, sampling_rate, waveforms, no_grad):\n",
        "        inputs = self.preprocessor(audios=waveforms, sampling_rate=sampling_rate, return_tensors='pt')\n",
        "        if no_grad:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.get_audio_features(**inputs)\n",
        "        else:\n",
        "            outputs = self.model.get_audio_features(**inputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "jlsen40qUNHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95rdDMPNGt4J",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Sample usage\n",
        "data = ImageAudioDataset(df_train, img_dir, audio_dir, sr=48000, max_sec=10)\n",
        "image_encoder = ImageEncoder(\"google/vit-base-patch16-224-in21k\")\n",
        "audio_encoder = AudioEncoder(\"laion/clap-htsat-unfused\")\n",
        "\n",
        "# Passing through only one example to test it\n",
        "waveform = data[0]['audio_raw']\n",
        "audio_embedding = audio_encoder.encode(\n",
        "    sampling_rate=48000,\n",
        "    waveforms=waveform, # No need to explicitly torch.unsqueeze this\n",
        "    no_grad=True\n",
        ")\n",
        "print(audio_embedding.shape)\n",
        "print(audio_embedding)\n",
        "\n",
        "image = data[0]['image']\n",
        "image_embedding = image_encoder.encode(\n",
        "    data=image,\n",
        "    no_grad=True\n",
        ")\n",
        "print(image_embedding.shape)\n",
        "print(image_embedding)\n",
        "\n",
        "# TODO:\n",
        "# class CLIP(torch.nn.Module)\n",
        "#   def __init__(self, image_encoder, audio_encoder):\n",
        "#       self.image_encoder = image_encoder\n",
        "#       self.audio_encoder = audio_encoder\n",
        "#       self.image_projection = torch.nn.Sequential([\n",
        "#           torch.nn.Linear(197 * 768, 197 * 768, bias=False),\n",
        "#           torch.nn.ReLU(),\n",
        "#           torch.nn.Linear(197 * 768, 512, bias=False))\n",
        "#       ])\n",
        "#       # TODO: Ensure dimensions here are correct and even better, don't hardcode them\n",
        "\n",
        "#   def forward(self, batch):\n",
        "#       image_embeddings = self.image_encoder.encode(batch.images) # Pass the images through the ViT\n",
        "#       audio_embeddings = self.audio_encoder.encode(batch.audio) # Pass the audio through CLAP\n",
        "#       projected_image_embeddings = self.image_projection(image_embeddings) # this is the MLP/Linear layer\n",
        "#       # TODO: In the repo code, normalization is applied here - we need to find out if it's necessary\n",
        "#       # Calculate loss (contrastive)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "# 1) hidden layer dim to be the same as input dim or twice as big\n",
        "# 2) image_encoder.encode(... no_grad=True) -> local/temporary\n",
        "#    image_encoder.model.parameters(): param.requires_grad = False -> global/permanent\n",
        "# 3) fix dimensions\n",
        "#"
      ],
      "metadata": {
        "id": "q-Lc_kmyPPbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPMapper(torch.nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_dim, h_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(h_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "csaP6x-UqEe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder = ImageEncoder(\"google/vit-base-patch16-224-in21k\")\n",
        "audio_encoder = AudioEncoder(\"laion/clap-htsat-unfused\")\n",
        "\n",
        "for param in image_encoder.model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in audio_encoder.model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "6Wobm3uTX1HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PictureToMusicModel(torch.nn.Module):\n",
        "    def __init__(self, im_dim, au_dim, h_dim, out_dim, im_en, au_en):\n",
        "        super().__init__()\n",
        "        self.image_encoder = im_en\n",
        "        self.audio_encoder = au_en\n",
        "        self.image_mapper = MLPMapper(im_dim, h_dim, out_dim)\n",
        "        self.audio_mapper = MLPMapper(au_dim, h_dim, out_dim)\n",
        "        self.logit_scale = torch.nn.Parameter(torch.tensor(1.0)) # learnable temperature\n",
        "\n",
        "    def forward(self, image_input, audio_input):\n",
        "        image_emb = self.image_encoder.encode(data=image_input, no_grad=True)\n",
        "        audio_emb = self.audio_encoder.encode(sampling_rate=48000, waveforms=audio_input, no_grad=True)\n",
        "\n",
        "        image_proj = self.image_mapper(image_emb)\n",
        "        audio_proj = self.audio_mapper(audio_emb)\n",
        "\n",
        "        image_proj = torch.nn.functional.normalize(image_proj, dim=-1)\n",
        "        audio_proj = torch.nn.functional.normalize(audio_proj, dim=-1)\n",
        "\n",
        "        # clamping (restricting) the temperature\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logit_scale = torch.clamp(logit_scale, 0, 100)\n",
        "\n",
        "        return image_proj, audio_proj, logit_scale"
      ],
      "metadata": {
        "id": "DmO1FWov9AOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "FLREVvvvXKCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im_dim = 768       # ViT output dim 197x768\n",
        "h_dim = im_dim * 2\n",
        "au_dim = 512       # CLAP output dim\n",
        "out_dim = 512      # Shared space\n",
        "\n",
        "model = PictureToMusicModel(im_dim, au_dim, h_dim, out_dim, image_encoder, audio_encoder)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "07hQc9kuYCTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(image_embeds, audio_embeds, logit_scale): #learnable temperature\n",
        "    logits_per_image = logit_scale * image_embeds @ audio_embeds.T\n",
        "    logits_per_audio = logits_per_image.T\n",
        "\n",
        "    labels = torch.arange(image_embeds.size(0)).to(image_embeds.device)\n",
        "\n",
        "    loss_i2a = F.cross_entropy(logits_per_image, labels)\n",
        "    loss_a2i = F.cross_entropy(logits_per_audio, labels)\n",
        "\n",
        "    return (loss_i2a + loss_a2i) / 2\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.image_mapper.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.audio_mapper.parameters(), 'lr': 1e-4},\n",
        "    {'params': [model.logit_scale], 'lr': 1e-4},\n",
        "], weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "pf3njfm99CN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, image_input, audio_input, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    image_input = image_input.to(device)\n",
        "    audio_input = audio_input.to(device)\n",
        "\n",
        "\n",
        "    image_proj, audio_proj, logit_scale = model(image_input, audio_input)\n",
        "    loss = contrastive_loss(image_proj, audio_proj, logit_scale)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Loss: {loss.item():.4f}\")\n",
        "    print(f\"Image shape: {image_input.shape}\")  # Expect [B, C, H, W]\n",
        "    print(f\"Audio shape: {audio_input.shape}\")  # Expect [B, T] or [B, 1, T]\n",
        "    print(f\"Image_proj shape: {image_proj.shape}\")  # [B, D]\n",
        "    print(f\"Audio_proj shape: {audio_proj.shape}\\n\",)  # [B, D]\n",
        "\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "banVQP7lXO0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, train_loader, val_loader, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            image_input = batch[\"image\"].to(device)\n",
        "            audio_input = batch[\"audio_raw\"].to(device)\n",
        "\n",
        "            loss = train_step(model, image_input, audio_input, optimizer)\n",
        "            train_loss += loss\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                image_input = batch[\"image\"].to(device)\n",
        "                audio_input = batch[\"audio_raw\"].to(device)\n",
        "\n",
        "                image_proj, audio_proj, logit_scale = model(image_input, audio_input)\n",
        "                loss = contrastive_loss(image_proj, audio_proj, logit_scale)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "l5IRxjdBbtSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop(model, train_loader, val_loader, optimizer, num_epochs=5)"
      ],
      "metadata": {
        "id": "SyVbTOxIgHev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "# 1. Add an MLP head on top of the CLIP embedding.\n",
        "# 2. Load image and audio encoders from existing CLIP-compatible models.\n",
        "# 3. Pass image/audio through their encoders and get embeddings.\n",
        "# 4. Concatenate or fuse the embeddings if needed.\n",
        "# 5. Train the MLP to map image embeddings to audio embeddings (or vice versa).\n"
      ],
      "metadata": {
        "id": "Ryt1CUsCpaK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}