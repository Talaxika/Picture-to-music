{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1897587,"sourceType":"datasetVersion","datasetId":1130776}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import ViTModel, ViTImageProcessorFast, ClapModel, ClapProcessor\nimport torch\nfrom torchvision import transforms\nfrom torch import nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, model_type, checkpoint):\n        super().__init__()\n        self.model_type = model_type\n\n        if self.model_type == 'vit':\n            # Load the model and the class that the inputs go through\n            self.model = ViTModel.from_pretrained(checkpoint, device_map='auto')\n            self.preprocessor = ViTImageProcessorFast.from_pretrained(checkpoint)\n        else:\n            raise Exception('Unsupported model for image encoder')\n\n    def encode(self, data):\n        if self.model_type == 'vit':\n            inputs = self.preprocessor(images = data, return_tensors=\"pt\", do_normalize=True, do_convert_rgb=True, do_rescale=True, do_resize=True)\n            inputs = inputs.to(device)\n            outputs = self.model(**inputs)\n            return outputs.pooler_output\n\nclass AudioEncoder(nn.Module):\n    def __init__(self, model_type, checkpoint):\n        super().__init__()\n        self.model_type = model_type\n\n        if self.model_type == 'clap':\n            # Load the model and the class that the inputs go through\n            self.model = ClapModel.from_pretrained(checkpoint, device_map='auto')\n            self.preprocessor = ClapProcessor.from_pretrained(checkpoint, use_fast=True)\n        else:\n            raise Exception('Unsupported model for audio encoder')\n\n    def encode(self, sampling_rate, waveforms):\n        if self.model_type == 'clap':\n            inputs = self.preprocessor(audios=waveforms.numpy(), sampling_rate=sampling_rate, return_tensors='pt')\n            inputs = inputs.to(device)\n            outputs = self.model.get_audio_features(**inputs)\n            return outputs\n\nclass MLPMapper(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.Tanh(),\n            torch.nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass PictureToMusicModel(torch.nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.init_image_encoder()\n        self.init_audio_encoder()\n\n        self.image_mapper = MLPMapper(\n            input_dim=self.config.image_embedding_size, \n            hidden_dim=self.config.mlp_hidden_size, \n            output_dim=self.config.shared_embedding_size\n        )\n        if config.has_audio_mapper:\n            self.audio_mapper = MLPMapper(\n                input_dim=self.config.audio_embedding_size,\n                hidden_dim=self.config.mlp_hidden_size,\n                output_dim=self.config.shared_embedding_size\n            )\n\n        # Learnable temperature\n        self.logit_scale = torch.nn.Parameter(torch.tensor(1.0))\n\n    def forward(self, image_input, audio_input):\n        image_emb = self.image_encoder.encode(data=image_input)\n        audio_emb = self.audio_encoder.encode(sampling_rate=48000, waveforms=audio_input)\n\n        image_proj = self.image_mapper(image_emb)\n        if self.config.has_audio_mapper:\n            audio_proj = self.audio_mapper(audio_emb)\n        else:\n            audio_proj = audio_emb\n\n        # clamping (restricting) the temperature\n        logit_scale = self.logit_scale.exp()\n        logit_scale = torch.clamp(logit_scale, 0, 100)\n\n        return image_proj, audio_proj, logit_scale\n\n    def encode_image(self, image_input):\n        image_emb = self.image_encoder.encode(data=image_input)\n        image_proj = self.image_mapper(image_emb)\n        image_proj = torch.nn.functional.normalize(image_proj, dim=-1)\n        res = image_proj.detach().cpu().numpy()\n        return res\n        \n    def encode_audio(self, audio_input):\n        audio_emb = self.audio_encoder.encode(sampling_rate=48000, waveforms=audio_input)\n        if self.config.has_audio_mapper:\n            audio_proj = self.audio_mapper(audio_emb)\n        else:\n            audio_proj = audio_emb\n        audio_proj = torch.nn.functional.normalize(audio_proj, dim=-1)\n        res = audio_proj.detach().cpu().numpy()\n        \n        return res\n        \n    def init_image_encoder(self):        \n        self.image_encoder = ImageEncoder(\n            model_type=self.config.image_encoder_type,\n            checkpoint=self.config.image_encoder_checkpoint\n        )\n        \n        for param in self.image_encoder.model.parameters():\n            param.requires_grad = False\n        if not self.config.freeze_image_encoder:\n            # Unfreeze only the last couple of layers so we can finetune\n            for param in self.image_encoder.model.encoder.layer[-self.config.num_layers_to_unfreeze].parameters():\n                param.requires_grad = True\n\n    def init_audio_encoder(self):\n        self.audio_encoder = AudioEncoder(\n            model_type=self.config.audio_encoder_type, \n            checkpoint=self.config.audio_encoder_checkpoint\n        )\n        if self.config.freeze_audio_encoder:\n            for param in self.audio_encoder.model.parameters():\n                param.requires_grad = False\n\n\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass PictureToMusicConfig:\n    image_encoder_type: str = 'vit'\n    image_encoder_checkpoint: str = 'google/vit-base-patch16-224-in21k'\n    audio_encoder_type: str = 'clap'\n    audio_encoder_checkpoint: str = 'laion/clap-htsat-unfused'\n    freeze_audio_encoder: bool = True\n    freeze_image_encoder: bool = True\n    num_layers_to_unfreeze: int = 1\n    image_embedding_size: int = 768\n    audio_embedding_size: int = 512\n    shared_embedding_size: int = 512\n    has_audio_mapper: bool = False\n    mlp_hidden_size: int = 1024","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load model from hugging face**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\n#from picture_to_music import PictureToMusicModel, PictureToMusicConfig\nimport torch, json\n\n# # Load config from hugging face\nconfig_path = hf_hub_download(\"Pesho564/Picture-to-music\", \"config.json\")\nwith open(config_path) as f:\n     config = json.load(f)\n\nconfig_class = PictureToMusicConfig(**config)\n\n# Load weights\nweights_path = hf_hub_download(\"Pesho564/Picture-to-music\", \"model_state_dict.bin\")\nmodel = PictureToMusicModel(config_class).to(device)\nmodel.load_state_dict(torch.load(weights_path))\n\nmodel.eval()\n\n# Model is now ready","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio_dir = '/kaggle/input/fma-free-music-archive-small-medium/fma_small/fma_small'\nsave_path = '/kaggle/working/audio_embeddings.pkl'\nsr = 48000 # HZ\nduration = 10 # seconds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:32:44.828884Z","iopub.execute_input":"2025-06-23T17:32:44.829088Z","iopub.status.idle":"2025-06-23T17:32:44.833060Z","shell.execute_reply.started":"2025-06-23T17:32:44.829071Z","shell.execute_reply":"2025-06-23T17:32:44.832212Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os \n\n# Count total files and subdirs\nn_files = 0\nn_dirs = 0\nfor root, dirs, files in os.walk(audio_dir):\n    n_files += len([f for f in files if f.endswith(\".mp3\")])\n    n_dirs += len(dirs)\nprint(f\"Total MP3 files: {n_files}\")\nprint(f\"Total subdirs: {n_dirs}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:32:44.834701Z","iopub.execute_input":"2025-06-23T17:32:44.834974Z","iopub.status.idle":"2025-06-23T17:33:00.813858Z","shell.execute_reply.started":"2025-06-23T17:32:44.834942Z","shell.execute_reply":"2025-06-23T17:33:00.812955Z"}},"outputs":[{"name":"stdout","text":"Total MP3 files: 8000\nTotal subdirs: 156\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def load_and_preprocess_audio(filepath, sr, duration):\n    y, _ = librosa.load(filepath, sr=sr, mono=True)\n    target_len = sr * duration\n    if len(y) < target_len:\n        # Pad with zeros if too short\n        y = np.pad(y, (0, target_len - len(y)))\n    elif len(y) > target_len:\n        # Trim if too long\n        y = y[:target_len]\n    return y\n\ndef embed(audio):\n    return model.encode_audio(audio)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:33:00.814740Z","iopub.execute_input":"2025-06-23T17:33:00.814971Z","iopub.status.idle":"2025-06-23T17:33:00.820083Z","shell.execute_reply.started":"2025-06-23T17:33:00.814952Z","shell.execute_reply":"2025-06-23T17:33:00.819104Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Convert music dataset into embeddings**","metadata":{}},{"cell_type":"code","source":"import librosa\nimport numpy as np\nimport pandas as pd\n\nif not os.path.exists(save_path):\n    audios = []\n    track_ids = []\n    track_to_filepath = {}\n\n    idx = 0\n    for root, _, files in os.walk(audio_dir):\n        for fname in files:\n            if fname.endswith(\".mp3\"):\n                try:\n                    filepath = os.path.join(root, fname)\n                    track_id = os.path.splitext(fname)[0]\n                    track_ids.append(track_id)\n                    track_to_filepath[track_id] = filepath\n                    \n                    audio = load_and_preprocess_audio(filepath, sr, duration)\n                    audios.append(audio)\n                    idx += 1\n                    if idx % 100 == 0: # Save only on 100 iterations or so\n                        embeddings = embed(torch.tensor(np.array(audios)))\n                        embeddings_df = pd.DataFrame(embeddings, index=track_ids)\n                        embeddings_df.to_pickle(save_path+str(idx))\n                        audios = []\n                        track_ids = []\n                        print('Saved')\n                except Exception as error:\n                    print(error)\n    \n    embeddings = embed(torch.tensor(np.array(audios)))\n    embeddings_df = pd.DataFrame(embeddings, index=track_ids)\n    embeddings_df.to_pickle(save_path)\nelse:\n    embeddings_df = pd.read_pickle(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:35:02.524575Z","iopub.execute_input":"2025-06-23T18:35:02.524903Z","iopub.status.idle":"2025-06-23T18:35:02.535394Z","shell.execute_reply.started":"2025-06-23T18:35:02.524880Z","shell.execute_reply":"2025-06-23T18:35:02.534632Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"if not os.path.exists(save_path):\n    # Since all of the audio is too big to fit into one pass through the model\n    # We split it into multiple passes and into multiple files and combine the results into a dataframe here\n    dfs = []\n    for _, _, files in os.walk('/kaggle/working'):\n        for f in files:\n            dfs.append(pd.read_pickle(f))\n    embeddings_df = pd.concat(dfs)\n    embeddings_df.to_pickle(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:33:29.246572Z","iopub.execute_input":"2025-06-23T18:33:29.247196Z","iopub.status.idle":"2025-06-23T18:33:29.270030Z","shell.execute_reply.started":"2025-06-23T18:33:29.247173Z","shell.execute_reply":"2025-06-23T18:33:29.269434Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"**Load a sample image**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport requests\n\nim = Image.open(requests.get('http://farm2.staticflickr.com/1357/947054324_da3b551fa9_z.jpg', stream=True).raw)\n\nimage_embedding = model.encode_image(np.array(im))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:34:21.637130Z","iopub.execute_input":"2025-06-23T18:34:21.637467Z","iopub.status.idle":"2025-06-23T18:34:21.938040Z","shell.execute_reply.started":"2025-06-23T18:34:21.637430Z","shell.execute_reply":"2025-06-23T18:34:21.937041Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"**Find and display the filename of the best music match**","metadata":{}},{"cell_type":"code","source":"def find_best_match_knn(query_emb, embeddings, k=5):\n    scores = embeddings_df.apply(lambda row: np.dot(query_emb, row.values).item(), axis=1)\n    return scores.nlargest(k)\n\ntop_matches = find_best_match_knn(image_embedding, embeddings_df, k=5)\nprint(top_matches)\nprint(track_to_filepath[top_matches.index[0]])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}